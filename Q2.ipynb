{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2\n",
    "\n",
    "## Q2.1\n",
    "When using linear activation f(z) = z in the hidden layer:\n",
    "\n",
    "    h₁ = w₁₁x₁ + w₁₂x₂ + b₁\n",
    "    h₂ = w₂₁x₁ + w₂₂x₂ + b₂\n",
    "\n",
    "Output neuron:\n",
    "\n",
    "    y = s(v₁h₁ + v₂h₂ + b₃)\n",
    "\n",
    "Substituting h₁ and h₂:\n",
    "\n",
    "    y = s((v₁w₁₁ + v₂w₂₁)x₁ + (v₁w₁₂ + v₂w₂₂)x₂ + (v₁b₁ + v₂b₂ + b₃))\n",
    "\n",
    "This is equal to a single perceptron with:\n",
    "\n",
    "- Weight for x₁: W₁ = v₁w₁₁ + v₂w₂₁\n",
    "- Weight for x₂: W₂ = v₁w₁₂ + v₂w₂₂\n",
    "- Bias: B = v₁b₁ + v₂b₂ + b₃\n",
    "\n",
    "Thus, with linear activation functions, the hidden layer collapses to a single layer, which can't learn XOR.\n",
    "\n",
    "## Q2.2\n",
    "\n",
    "Using ReLU activation f(z) = max{z,0}:\n",
    "\n",
    "First hidden neuron (h₁):\n",
    "- w₁₁ = 1  (weight for x₁)\n",
    "- w₁₂ = 1  (weight for x₂)\n",
    "- b₁ = -0.5 (bias)\n",
    "\n",
    "Second hidden neuron (h₂):\n",
    "- w₂₁ = -1  (weight for x₁)\n",
    "- w₂₂ = -1  (weight for x₂)\n",
    "- b₂ = 1.5  (bias)\n",
    "\n",
    "Output perceptron:\n",
    "- v₁ = 1    (weight for h₁)\n",
    "- v₂ = -1   (weight for h₂)\n",
    "- b₃ = -0.5 (bias)\n",
    "\n",
    "Checking:\n",
    "\n",
    "- (0,0): h₁ = 0, h₂ = 1.5 → output = 0\n",
    "- (0,1): h₁ = 0.5, h₂ = 0.5 → output = 1\n",
    "- (1,0): h₁ = 0.5, h₂ = 0.5 → output = 1\n",
    "- (1,1): h₁ = 1.5, h₂ = 0 → output = 0\n",
    "\n",
    "-> XOR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
