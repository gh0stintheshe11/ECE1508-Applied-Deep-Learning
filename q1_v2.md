# Question 1: One-dimensional Convolution

## Q1.1

For a general integer stride $S$, the convolution produces an output $\mathbf{z} \in \mathbb{R}^{\lfloor \frac{N-F}{S} \rfloor + 1}$ where each entry is computed as:

$z_i = \mathbf{w}^T\mathbf{x}[1 + (i-1)S : F + (i-1)S]$

for $i = 1, 2, ..., \lfloor \frac{N-F}{S} \rfloor + 1$.

## Q1.2

For stride $S = \frac{1}{m}$ where $m$ is an integer, upsampling occurs before convolution:

1. The upsampled vector $\tilde{\mathbf{x}}$ is created by inserting $(m-1)$ zeros between each adjacent pair of elements in $\mathbf{x}$:
   - $\tilde{x}_{m \cdot (i-1) + 1} = x_i$ for $i = 1, 2, ..., N$
   - $\tilde{x}_j = 0$ for all other indices $j$

2. The upsampled vector $\tilde{\mathbf{x}}$ has length $(m \cdot N) - m + 1$

3. Standard convolution with stride 1 is applied on $\tilde{\mathbf{x}}$:
   $z_i = \mathbf{w}^T\tilde{\mathbf{x}}[i : F + i - 1]$

The output dimension is $\mathbf{z} \in \mathbb{R}^{m \cdot N - m - F + 2}$.

## Q1.3

Maintaining the same dimension after convolution (output length = $N$) requires padding:

For stride $S = 1$:
- Total padding required: $F-1$ zeros
- For odd $F$: $\frac{F-1}{2}$ zeros on each side
- For even $F$: $\frac{F}{2}-1$ zeros at the beginning and $\frac{F}{2}$ at the end

For a general stride $S$, the padding $P$ (equal on both sides) must satisfy:

$\left\lfloor \frac{N + 2P - F}{S} \right\rfloor + 1 = N$

Solving for $P$:
$P = \frac{(N-1) \cdot S + F - N}{2}$

## Q1.4

Using the chain rule:

$\nabla_\mathbf{x} \hat{R} = \text{Conv}(\nabla_\mathbf{z} \hat{R}|\hat{\mathbf{w}})$

where $\hat{\mathbf{w}}$ is the kernel $\mathbf{w}$ flipped. 

The description of backpropagation through a convolutional layer, where the gradient of the loss with respect to the output is convolved with the reversed filter.

## Q1.5

The gradient with respect to the kernel weights:

$\nabla_\mathbf{w} \hat{R} = \sum_{i=1}^{N-F+1} (\nabla_\mathbf{z} \hat{R})_i \cdot \mathbf{x}[i : F + i - 1]$

This can be expressed as the convolution:

$\nabla_\mathbf{w} \hat{R} = \text{Conv}(\mathbf{x}|\nabla_\mathbf{z} \hat{R})$

## Q1.6

The results are consistent with Chapter 4's lecture notes on multi-channel convolution:

1. Part 4 shows $\nabla_\mathbf{x} \hat{R} = \text{Conv}(\nabla_\mathbf{z} \hat{R}|\hat{\mathbf{w}})$, matching Chapter 4's formula where backpropagation to the input involves convolving the output gradient with the reversed filter.

2. Part 5 shows $\nabla_\mathbf{w} \hat{R} = \text{Conv}(\mathbf{x}|\nabla_\mathbf{z} \hat{R})$, matching Chapter 4's formula where the gradient with respect to the filter is computed by convolving the input with the gradient of the loss with respect to the output.

The one-dimensional case derived here is a special case of the more general multi-channel convolution formulas discussed in Chapter 4, demonstrating the consistency of backpropagation across different convolution implementations.