# Q1.1: Using \( \ln(1 + e^z) \)

Define the input vector including the bias term:

\[
y_0 = \begin{bmatrix} 1 \\ x_1 \\ x_2 \end{bmatrix} =
\begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}
\]

## Hidden Layer Computation
Compute \( z_1 = W_1 y_0 \)
\[
z_1[1] = 0.1 \times 1 + 0.1 \times 1 + 0.1 \times 1 = 0.3000
\]
\[
z_1[2] = 0.1 \times 1 + 0.1 \times 1 + 0.1 \times 1 = 0.3000
\]

Apply Soft-ReLU Activation: \( f(z) = \ln(1 + e^z) \)

For \( z_1[1] = 0.3000 \):

\[
e^{0.3000} = 1.3499
\]

\[
y_1[1] = \ln(1 + 1.3499) = \ln(2.3499) = 0.8544
\]

Same thing for \( z_1[2] = 0.3000 \):

\[
y_1[2] = \ln(1 + 1.3499) = 0.8544
\]

Add the bias term:

\[
y_1 = \begin{bmatrix} 1.0000 \\ 0.8544 \\ 0.8544 \end{bmatrix}
\]

### Output Layer
Compute \( z_2 = W_2 y_1 \)

\[
z_2 = 0.1 \times 1.0000 + 0.1 \times 0.8544 + 0.1 \times 0.8544
\]

\[
z_2 = 0.1000 + 0.0854 + 0.0854 = 0.2709
\]

Apply Sigmoid Activation: \( \sigma(z) = \frac{1}{1 + e^{-z}} \)

\[
y_2 = \frac{1}{1 + e^{-0.2709}}
\]

\[
= \frac{1}{1 + 0.7628} = \frac{1}{1.7628} = 0.5673
\]

Thus,

- Hidden layer:
  - \( z_1 = \begin{bmatrix} 0.3000 \\ 0.3000 \end{bmatrix} \)
  - \( y_1 = \begin{bmatrix} 1.0000 \\ 0.8544 \\ 0.8544 \end{bmatrix} \)
- Output layer:
  - \( z_2 = 0.2709 \)
  - \( y_2 = 0.5673 \)

---

# Q1.2

From Q1.1,
- \( y_1 = \begin{bmatrix} 1.0000 \\ 0.8544 \\ 0.8544 \end{bmatrix} \)
- \( z_2 = 0.2709 \)
- \( y_2 = 0.5673 \)


### Gradients for the Output Layer
##### Gradient of Loss with Respect to \( y_2 \)
Using the loss function \( L = -\log y_2 \):

\[
\frac{\partial L}{\partial y_2} = -\frac{1}{y_2} = -\frac{1}{0.5673} = -1.7626
\]

##### Gradient of \( y_2 \) with Respect to \( z_2 \)
Since \( y_2 = \sigma(z_2) \), use the sigmoid derivative:

\[
\frac{\partial y_2}{\partial z_2} = y_2 (1 - y_2) = 0.5673 \times (1 - 0.5673)
\]

\[
= 0.5673 \times 0.4327 = 0.2456
\]

Compute \( \delta z_2 \):

\[
\delta z_2 = \left(\frac{\partial L}{\partial y_2}\right) \left(\frac{\partial y_2}{\partial z_2}\right)
\]

\[
= -1.7626 \times 0.2456 = -0.4328
\]

Gradient for \( z_2 \): \( \delta z_2 = -0.4328 \)

### Compute Gradients for \( W_2 \)

\[
\nabla W_2 L = \delta z_2 y_1^T
\]

Each weight update:

\[
\frac{\partial L}{\partial W_2[1,0]} = -0.4328 \times 1.0000 = -0.4328
\]

\[
\frac{\partial L}{\partial W_2[1,1]} = -0.4328 \times 0.8544 = -0.3698
\]

\[
\frac{\partial L}{\partial W_2[1,2]} = -0.4328 \times 0.8544 = -0.3698
\]

Gradient for \( W_2 \):

\[
\nabla W_2 L = \begin{bmatrix} -0.4328 & -0.3698 & -0.3698 \end{bmatrix}
\]

### Compute Gradients for the Hidden Layer
Compute \( \delta y_1 \) using the chain rule:

\[
\delta y_1 = W_2^T \delta z_2
\]

Since \( W_2 = \begin{bmatrix} 0.1 & 0.1 & 0.1 \end{bmatrix} \):

\[
\delta y_1[1] = 0.1 \times (-0.4328) = -0.0433
\]

\[
\delta y_1[2] = 0.1 \times (-0.4328) = -0.0433
\]

### Compute \( \delta z_1 \) Using Soft-ReLU Derivative
The soft-ReLU derivative is:

\[
\frac{\partial f}{\partial z_1} = \frac{e^z}{1 + e^z}
\]

For \( z_1 = 0.3000 \):

\[
\frac{\partial f}{\partial z_1} = \frac{e^{0.3000}}{1 + e^{0.3000}}
\]

\[
= \frac{1.3499}{2.3499} = 0.5744
\]

\[
\delta z_1[1] = -0.0433 \times 0.5744 = -0.0249
\]

\[
\delta z_1[2] = -0.0433 \times 0.5744 = -0.0249
\]

Gradient for \( z_1 \):

\[
\delta z_1 = \begin{bmatrix} -0.0249 \\ -0.0249 \end{bmatrix}
\]

### Compute Gradients for \( W_1 \)
Using:

\[
\nabla W_1 L = \delta z_1 y_0^T
\]

Since \( y_0 = \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix} \), each weight update:

\[
\frac{\partial L}{\partial W_1[1,0]} = -0.0249 \times 1 = -0.0249
\]

\[
\frac{\partial L}{\partial W_1[1,1]} = -0.0249 \times 1 = -0.0249
\]

\[
\frac{\partial L}{\partial W_1[1,2]} = -0.0249 \times 1 = -0.0249
\]

\[
\frac{\partial L}{\partial W_1[2,0]} = -0.0249 \times 1 = -0.0249
\]

\[
\frac{\partial L}{\partial W_1[2,1]} = -0.0249 \times 1 = -0.0249
\]

\[
\frac{\partial L}{\partial W_1[2,2]} = -0.0249 \times 1 = -0.0249
\]

Gradient for \( W_1 \):

\[
\nabla W_1 L = \begin{bmatrix} -0.0249 & -0.0249 & -0.0249 \\ -0.0249 & -0.0249 & -0.0249 \end{bmatrix}
\]

Thus,

- Output layer \( W_2 \):
  \[
  \begin{bmatrix} -0.4328 & -0.3698 & -0.3698 \end{bmatrix}
  \]
- Hidden layer \( W_1 \):
  \[
  \begin{bmatrix} -0.0249 & -0.0249 & -0.0249 \\ -0.0249 & -0.0249 & -0.0249 \end{bmatrix}
  \]

---

# Q1.3

### Gradient Descent Update Rule
Use gradient descent:

\[
w \leftarrow w - \eta \nabla L
\]

where:
- \( \eta = 0.1 \) (learning rate)
- Gradients from Q1.2:
  - Output layer gradients: \( \begin{bmatrix} -0.4328, -0.3698, -0.3698 \end{bmatrix} \)
  - Hidden layer gradients: All values are \( -0.0249 \).
- Initial Weights:
  - \( W_2 = \begin{bmatrix} 0.1, 0.1, 0.1 \end{bmatrix} \)
  - \( W_1 = \begin{bmatrix} 0.1 & 0.1 & 0.1 \\ 0.1 & 0.1 & 0.1 \end{bmatrix} \)


### Compute Output Layer Updates \( W_2 \)
Weight update:

\[
W_{\text{new}} = W_{\text{old}} - \eta \cdot \nabla L
\]

- Bias update:

\[
W_2[1,0] = 0.1 - 0.1 \times (-0.4328)
\]

\[
= 0.1 + 0.0433 = 0.1433
\]

- Weight updates:

\[
W_2[1,1] = 0.1 - 0.1 \times (-0.3698)
\]

\[
= 0.1 + 0.0370 = 0.1370
\]

\[
W_2[1,2] = 0.1 - 0.1 \times (-0.3698)
\]

\[
= 0.1 + 0.0370 = 0.1370
\]

Updated Output Layer Weights:

\[
W_2 = \begin{bmatrix} 0.1433, 0.1370, 0.1370 \end{bmatrix}
\]

### Compute Hidden Layer Updates \( W_1 \)
Since all gradients = \( -0.0249 \) and the initial weights = \( 0.1 \),

\[
W_1[i,j] = 0.1 - 0.1 \times (-0.0249)
\]

\[
= 0.1 + 0.0025 = 0.1025
\]

Since this applies to all entries in \( W_1 \):

\[
W_1 = \begin{bmatrix} 0.1025 & 0.1025 & 0.1025 \\ 0.1025 & 0.1025 & 0.1025 \end{bmatrix}
\]

Updated Hidden Layer Weights:

\[
W_1 = \begin{bmatrix} 0.1025 & 0.1025 & 0.1025 \\ 0.1025 & 0.1025 & 0.1025 \end{bmatrix}
\]
