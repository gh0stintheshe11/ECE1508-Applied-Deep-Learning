# Q2.1

```
          Matrix A            Function L
z ------> Az + b ------> y --------> R
          ↑
          b
```

```
[Input z]     [Affine Transform]     [Output y]     [Scalar R]
z ∈ ℝᴺ ----+
           |   y = Az + b            y ∈ ℝᴷ         R = L(y)
b ∈ ℝᴷ ----+-----------------> y ------------> R
[Bias]
```

---

# Q2.2

### 1. Backpropagation
\[
y = A z + b
\]

- weight matrix \( A \in \mathbb{R}^{K \times N} \) .
- input vector \( z \in \mathbb{R}^{N} \) .
- bias vector \( b \in \mathbb{R}^{K} \) .

Scalar function \( \hat{R} \), which depends on \( y \):

\[
\hat{R} = L(y)
\]

Gradient of \( \hat{R} \) with respect to \( y \):

\[
\nabla_y \hat{R}
\]

### 2. Chain Rule
\( y \) is an affine function of \( b \), the derivative with respect to \( b \) is:

\[
\frac{\partial y}{\partial b} = I_K
\]

where \( I_K \) is the identity matrix of size \( K \times K \).

Use the chain rule:

\[
\nabla_b \hat{R} = \left( \frac{\partial y}{\partial b} \right)^T \nabla_y \hat{R}
\]

Since \( \frac{\partial y}{\partial b} = I_K \),

\[
\nabla_b \hat{R} = I_K \nabla_y \hat{R}
\]

Thus,

\[
\nabla_b \hat{R} = \nabla_y \hat{R}
\]
