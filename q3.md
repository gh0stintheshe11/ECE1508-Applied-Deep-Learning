## Q3.1

Softmax transformation:
   \[
   y_j = \frac{e^{z_j}}{\Pi(z)}
   \]
   where:
   \[
   \Pi(z) = \sum_{i=1}^{N} e^{z_i}
   \]
   
Cross-Entropy Loss:
   \[
   \hat{R} = CE(y; p) = - \sum_{j=1}^{N} p_j \log y_j
   \]

### 1.Substitute \( y_j \) into the Cross-Entropy Formula

From the softmax function:

\[
y_j = \frac{e^{z_j}}{\sum_{i=1}^{N} e^{z_i}}
\]

Substituting this into the cross-entropy definition:

\[
\hat{R} = - \sum_{j=1}^{N} p_j \log \left( \frac{e^{z_j}}{\sum_{i=1}^{N} e^{z_i}} \right)
\]

### 2. Expand the Log

Use the log property:

\[
\log \frac{a}{b} = \log a - \log b
\]

\[
\hat{R} = - \sum_{j=1}^{N} p_j \left( z_j - \log \sum_{i=1}^{N} e^{z_i} \right)
\]

Expand the sum:

\[
\hat{R} = - \sum_{j=1}^{N} p_j z_j + \sum_{j=1}^{N} p_j \log \sum_{i=1}^{N} e^{z_i}
\]

### 3. Factor Out the Log Term
Since \( \log \sum_{i=1}^{N} e^{z_i} \) does not depend on \( j \), factor it out:

\[
\hat{R} = - \sum_{j=1}^{N} p_j z_j + \log \sum_{i=1}^{N} e^{z_i} \sum_{j=1}^{N} p_j
\]

Since probabilities sum to 1, i.e.,

\[
\sum_{j=1}^{N} p_j = 1
\]

the equation simplifies to:

\[
\hat{R} = - \sum_{j=1}^{N} p_j z_j + \log \sum_{i=1}^{N} e^{z_i}
\]

Thus,

\[
\hat{R} = - p^T z + \log \sum_{i=1}^{N} e^{z_i}
\]

---

# Q3.2

From Q3.1,

\[
\hat{R} = - p^T z + \log \sum_{i=1}^{N} e^{z_i}
\]

### 1. First Term: \( - p^T z \)
Since \( p^T z = \sum_{j=1}^{N} p_j z_j \), its gradient w.r.t. \( z \) is:

\[
\nabla_z (- p^T z) = - p
\]


### 2. Second Term: \( \log \sum_{i=1}^{N} e^{z_i} \)
Differentiate:

\[
\log \sum_{i=1}^{N} e^{z_i}
\]

Use the chain rule:

\[
\nabla_z \log \sum_{i=1}^{N} e^{z_i} = \frac{1}{\sum_{i=1}^{N} e^{z_i}} \nabla_z \sum_{i=1}^{N} e^{z_i}
\]

Since \( \frac{\partial}{\partial z_j} \sum_{i=1}^{N} e^{z_i} = e^{z_j} \),

\[
\nabla_z \log \sum_{i=1}^{N} e^{z_i} = \frac{e^{z_1}}{\sum_{i=1}^{N} e^{z_i}}, \frac{e^{z_2}}{\sum_{i=1}^{N} e^{z_i}}, ..., \frac{e^{z_N}}{\sum_{i=1}^{N} e^{z_i}}
\]

Softmax function \( y_j \):

\[
\nabla_z \log \sum_{i=1}^{N} e^{z_i} = y
\]

### 3. Combine

Combining both terms:

\[
\nabla_z \hat{R} = - p + y
\]

Thus,

\[
\nabla_z \hat{R} = y - p
\]

---

# Q3.3

From Q3.2,

\[
\nabla_z \hat{R} = y - p
\]

where:
- \( y \) is the softmax output:
  \[
  y_j = \frac{e^{z_j}}{\sum_{i=1}^{N} e^{z_i}}
  \]
- \( p \) is the given probability distribution in the cross-entropy function.

Thus,

\[
\nabla_z \hat{R} = y - p
\]
