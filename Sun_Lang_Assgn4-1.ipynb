{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42052c63",
   "metadata": {},
   "source": [
    "# Assignment 4: RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9abe0cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is used to include model diagram in this file. It is not related to the implementation\n",
    "from IPython.display import Image\n",
    "\n",
    "# torch modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader \n",
    "\n",
    "# Dataset from torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "# We need them for plot (not really asked, but if you would like to plot)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96523741-ee34-47d3-8916-83a76205bed0",
   "metadata": {},
   "source": [
    "First and for most we load the MNIST dataset and break it doen into mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49b4baf3-e887-4f21-82e0-12eb53a6793e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "\n",
    "# transformations\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# load training dataset\n",
    "trainset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# load test dataset\n",
    "testset = MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49e7f94-9c8a-48c5-baa6-425e460d92ed",
   "metadata": {},
   "source": [
    "We are going to implement the following RNN for MNIST classification via various recurrent units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d08d05f2-911d-48f4-950b-6e3e5b8cc230",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file or directory: 'RNN.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/ECE1508-Applied-Deep-Learning/.conda/lib/python3.11/site-packages/IPython/core/display.py:1128\u001b[0m, in \u001b[0;36mImage._data_and_metadata\u001b[0;34m(self, always_both)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1128\u001b[0m     b64_data \u001b[38;5;241m=\u001b[39m \u001b[43mb2a_base64\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mascii\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/ECE1508-Applied-Deep-Learning/.conda/lib/python3.11/site-packages/IPython/core/formatters.py:1036\u001b[0m, in \u001b[0;36mMimeBundleFormatter.__call__\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m   1033\u001b[0m     method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n\u001b[1;32m   1035\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1036\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/ECE1508-Applied-Deep-Learning/.conda/lib/python3.11/site-packages/IPython/core/display.py:1118\u001b[0m, in \u001b[0;36mImage._repr_mimebundle_\u001b[0;34m(self, include, exclude)\u001b[0m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed:\n\u001b[1;32m   1117\u001b[0m     mimetype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mimetype\n\u001b[0;32m-> 1118\u001b[0m     data, metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_and_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43malways_both\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m metadata:\n\u001b[1;32m   1120\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m {mimetype: metadata}\n",
      "File \u001b[0;32m~/ECE1508-Applied-Deep-Learning/.conda/lib/python3.11/site-packages/IPython/core/display.py:1130\u001b[0m, in \u001b[0;36mImage._data_and_metadata\u001b[0;34m(self, always_both)\u001b[0m\n\u001b[1;32m   1128\u001b[0m     b64_data \u001b[38;5;241m=\u001b[39m b2a_base64(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mascii\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1131\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo such file or directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m md \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: No such file or directory: 'RNN.png'"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file or directory: 'RNN.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/ECE1508-Applied-Deep-Learning/.conda/lib/python3.11/site-packages/IPython/core/display.py:1128\u001b[0m, in \u001b[0;36mImage._data_and_metadata\u001b[0;34m(self, always_both)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1128\u001b[0m     b64_data \u001b[38;5;241m=\u001b[39m \u001b[43mb2a_base64\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mascii\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/ECE1508-Applied-Deep-Learning/.conda/lib/python3.11/site-packages/IPython/core/formatters.py:406\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    404\u001b[0m     method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/ECE1508-Applied-Deep-Learning/.conda/lib/python3.11/site-packages/IPython/core/display.py:1150\u001b[0m, in \u001b[0;36mImage._repr_png_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_repr_png_\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FMT_PNG:\n\u001b[0;32m-> 1150\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_and_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ECE1508-Applied-Deep-Learning/.conda/lib/python3.11/site-packages/IPython/core/display.py:1130\u001b[0m, in \u001b[0;36mImage._data_and_metadata\u001b[0;34m(self, always_both)\u001b[0m\n\u001b[1;32m   1128\u001b[0m     b64_data \u001b[38;5;241m=\u001b[39m b2a_base64(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mascii\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1131\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo such file or directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m md \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: No such file or directory: 'RNN.png'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(\"RNN.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8ffbec",
   "metadata": {},
   "source": [
    "## Question 1: Implementing Basic RNN\n",
    "We implement the RNN via basic Recurrent Unit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab1b42f",
   "metadata": {},
   "source": [
    "### Building our Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6a672d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myRNN(nn.Module):\n",
    "    def __init__(self, batch_size):\n",
    "        super(myRNN, self).__init__()\n",
    "        self.h_size = 150  # size of hidden state as mentioned in the problem\n",
    "        self.time = 28     # MNIST images are 28x28, so sequence length is 28\n",
    "        self.x_size = 28   # each time step has 28 features (one row of pixels)\n",
    "        self.y_size = 10   # 10 output classes (digits 0-9)\n",
    "\n",
    "        # we need batch-size\n",
    "        # since we should make same-size for initial state\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.RecUnit = nn.RNN(self.x_size, self.h_size, batch_first=True)\n",
    "        # don't forget setting: batch_first = True\n",
    "\n",
    "        self.output_layer = nn.Linear(self.h_size, self.y_size)\n",
    "    def initial_state(self):\n",
    "         return torch.zeros(1, self.batch_size, self.h_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = self.initial_state()\n",
    "        output, hn = self.RecUnit(x, h0)\n",
    "        # Get the last time step's output\n",
    "        last_output = output[:, -1, :]\n",
    "        # Pass through output layer\n",
    "        y = self.output_layer(last_output)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0c964e",
   "metadata": {},
   "source": [
    "### Write Accuracy Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "025fad8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(z_out, labels):\n",
    "    # z_out and labels are both a mini-batch\n",
    "    correct_pred = (torch.argmax(z_out, dim=1) == labels).float().mean()\n",
    "    accuracy = 100.0 * correct_pred\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15153ac7",
   "metadata": {},
   "source": [
    "### Implement Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "409698b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss_function, num_epochs):\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    for epoch in range(num_epochs): \n",
    "        train_loss = 0.0\n",
    "        train_accuracy = 0.0\n",
    "\n",
    "        # training loop\n",
    "        for i, data in enumerate(trainloader):\n",
    "            # reset initial states\n",
    "            model.h_state = model.initial_state() \n",
    "\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "\n",
    "            # reshape input\n",
    "            inputs = inputs.view(inputs.size(0), 28, 28)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # compute loss\n",
    "            loss = loss_function(outputs, labels)\n",
    "\n",
    "            # backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # update training loss and accuracy\n",
    "            train_loss += loss.item()\n",
    "            train_accuracy += accuracy(outputs, labels)\n",
    "\n",
    "        print('Epoch:%d|Loss:%.4f|Accuracy:%.2f' \n",
    "        %(epoch+1, train_loss / (i+1), train_accuracy / (i+1)))\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb4a640",
   "metadata": {},
   "source": [
    "and now we can train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1d76d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1|Loss:0.7224|Accuracy:76.55\n",
      "Epoch:2|Loss:0.3034|Accuracy:91.18\n",
      "Epoch:3|Loss:0.2241|Accuracy:93.44\n",
      "Epoch:4|Loss:0.1874|Accuracy:94.65\n",
      "Epoch:5|Loss:0.1707|Accuracy:95.17\n",
      "Epoch:6|Loss:0.1501|Accuracy:95.78\n",
      "Epoch:7|Loss:0.1415|Accuracy:96.01\n",
      "Epoch:8|Loss:0.1455|Accuracy:95.88\n",
      "Epoch:9|Loss:0.1300|Accuracy:96.31\n",
      "Epoch:10|Loss:0.1227|Accuracy:96.51\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model\n",
    "model = myRNN(batch_size)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "train(model, loss_function, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6449562",
   "metadata": {},
   "source": [
    "## Question 2: Implementing RNN with Gated Unit\n",
    "We now replace basic recurrent unit with a GRU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b0289e",
   "metadata": {},
   "source": [
    "### Building RNN with GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87abdd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myGatedRNN(nn.Module):\n",
    "    def __init__(self, batch_size):\n",
    "        super(myGatedRNN, self).__init__()\n",
    "        self.h_size = 150\n",
    "        self.time = 28\n",
    "        self.x_size = 28\n",
    "        self.y_size = 10\n",
    "\n",
    "        # we need batch-size\n",
    "        # since we should make same-size for initial state\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.RecUnit = nn.GRU(self.x_size, self.h_size, batch_first=True)\n",
    "        # don't forget setting: batch_first = True\n",
    "\n",
    "        self.output_layer = nn.Linear(self.h_size, self.y_size)\n",
    "        \n",
    "    def initial_state(self):\n",
    "        return torch.zeros(1, self.batch_size, self.h_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = self.initial_state()\n",
    "        output, hn = self.RecUnit(x, h0)\n",
    "        # Get the last time step's output\n",
    "        last_output = output[:, -1, :]\n",
    "        # Pass through output layer\n",
    "        y = self.output_layer(last_output)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dde3e75",
   "metadata": {},
   "source": [
    "### Train Gated RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64a79b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1|Loss:0.4712|Accuracy:84.46\n",
      "Epoch:2|Loss:0.1240|Accuracy:96.30\n",
      "Epoch:3|Loss:0.0789|Accuracy:97.68\n",
      "Epoch:4|Loss:0.0620|Accuracy:98.12\n",
      "Epoch:5|Loss:0.0473|Accuracy:98.54\n",
      "Epoch:6|Loss:0.0393|Accuracy:98.78\n",
      "Epoch:7|Loss:0.0344|Accuracy:98.96\n",
      "Epoch:8|Loss:0.0296|Accuracy:99.07\n",
      "Epoch:9|Loss:0.0250|Accuracy:99.23\n",
      "Epoch:10|Loss:0.0210|Accuracy:99.36\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model\n",
    "model = myGatedRNN(batch_size)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "train(model, loss_function, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea61873a",
   "metadata": {},
   "source": [
    "## Question 3: Implementing RNN with LSTM\n",
    "We now replace basic recurrent unit with an LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e19c7a7",
   "metadata": {},
   "source": [
    "### Building RNN with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65c0f0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myLSTM(nn.Module):\n",
    "    def __init__(self, batch_size):\n",
    "        super(myLSTM, self).__init__()\n",
    "        self.h_size = 150\n",
    "        self.time = 28\n",
    "        self.x_size = 28\n",
    "        self.y_size = 10 \n",
    "\n",
    "        # we need batch-size\n",
    "        # since we should make same-size for initial state\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.RecUnit = nn.LSTM(self.x_size, self.h_size, batch_first=True)\n",
    "        # don't forget setting: batch_first = True\n",
    "\n",
    "        self.output_layer = nn.Linear(self.h_size, self.y_size)\n",
    "        \n",
    "    def initial_state(self):\n",
    "        # LSTM needs both hidden state and cell state\n",
    "        h0 = torch.zeros(1, self.batch_size, self.h_size)\n",
    "        c0 = torch.zeros(1, self.batch_size, self.h_size)\n",
    "        return h0, c0\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0, c0 = self.initial_state()\n",
    "        output, (hn, cn) = self.RecUnit(x, (h0, c0))\n",
    "        # Get the last time step's output\n",
    "        last_output = output[:, -1, :]\n",
    "        # Pass through output layer\n",
    "        y = self.output_layer(last_output)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24db6719",
   "metadata": {},
   "source": [
    "### Train LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f92e4fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1|Loss:0.4949|Accuracy:84.00\n",
      "Epoch:2|Loss:0.1493|Accuracy:95.56\n",
      "Epoch:3|Loss:0.0973|Accuracy:97.06\n",
      "Epoch:4|Loss:0.0748|Accuracy:97.76\n",
      "Epoch:5|Loss:0.0599|Accuracy:98.21\n",
      "Epoch:6|Loss:0.0507|Accuracy:98.44\n",
      "Epoch:7|Loss:0.0429|Accuracy:98.68\n",
      "Epoch:8|Loss:0.0358|Accuracy:98.92\n",
      "Epoch:9|Loss:0.0321|Accuracy:99.00\n",
      "Epoch:10|Loss:0.0271|Accuracy:99.18\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model\n",
    "model = myLSTM(batch_size)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "train(model, loss_function, num_epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
